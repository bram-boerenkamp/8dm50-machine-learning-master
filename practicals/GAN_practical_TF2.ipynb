{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dEjdfhRZsUGR",
        "S8CchPs2kIVT",
        "RBALpqMLFPt2",
        "dqKCNciykQ-2"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1rhzu_KRVfD"
      },
      "source": [
        "# Data synthesis using generative adversarial networks (GANs)\n",
        "\n",
        "In this practical session we will build and train generative adversarial networks that are able to generate images.\n",
        "\n",
        "This is a Google Colab notebook, which is mostly the same as a Jupyter notebook. This means that you can run the code in a cell by selecting the cell and pressing Shift+Enter. We will run everything in the cloud, so you don't need a fancy computer or an expensive GPU for this exercise. We are going to use GPUs that Google Cloud provides for free. To do this, go to Edit --> Notebook settings and select GPU as Hardware accelerator. Then, in the top right of this screen select 'CONNECT' --> 'Connect to hosted runtime'\n",
        "\n",
        "We are going to import some of the packages that we will need in this exercise (by running the cell below)\n",
        "\n",
        "The documentation for key packages can be found online: <br>\n",
        "For numpy: https://docs.scipy.org/doc/numpy-dev/user/quickstart.html <br>\n",
        "For matplotlib: http://matplotlib.org/api/pyplot_api.html <br>\n",
        "For Keras: https://keras.io/ <br>\n",
        "For random: https://docs.python.org/2/library/random.html <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV9fYTQRRQ6J",
        "outputId": "a70ca76e-4038-45d8-8eb9-b52a5b4f6072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pickle\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib import rc\n",
        "from matplotlib import pylab\n",
        "from IPython import display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "import time\n",
        "import random\n",
        "from google.colab import files\n",
        "from google.colab import widgets\n",
        "random.seed(0)\n",
        "\n",
        "# Check whether we're using a GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9hTkVp1ESQW"
      },
      "source": [
        "## Small steps\n",
        "Before moving on to generating images, we will start with a simple 1D problem. We will assume that there is a data set of real samples that are drawn from a normal distribution with a particular mean value (the black dotted line below). These samples are in the sample domain *x*.\n",
        "\n",
        "The generator network does not know anything about the distribution of the real samples in the sample domain *x*, but will try to come up with a transformation that maps random noise from a distribution *z* to samples that seem to come from the real sample distribution (the green line below). This is very similar to what we have looked at in the lecture.\n",
        "\n",
        "<img src=\"https://cs.stanford.edu/people/karpathy/gan/gan.png\">\n",
        "\n",
        "For this, we will define two neural networks that play a game:\n",
        "*   The discriminator will learn to distinguish real and fake samples in *z*\n",
        "*   The generator will generate fake samples in *z* that the discriminator cannot discriminate\n",
        "\n",
        "First, we determine the mean value of the normal distribution from which **real** samples will be drawn in the sample domain *x*. In addition, we define the dimensionality of the normal distribution *z* from which noise samples to the generator will be drawn, i.e. the latent space. This will be 1 for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsg4CLVHgMeJ"
      },
      "source": [
        "real_mean = 8\n",
        "latent_dim = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eLRuaRXgSKn"
      },
      "source": [
        "Next, we define our discriminator and generator. These are both very simple networks.\n",
        "\n",
        "**Question** How many layers does each of these networks have?\n",
        "\n",
        "**Answer** The discriminator network has 1 input layer, 1 hidden layer and 1 output layer. It's hidden layer is a Dense layer of 32 neurons.\n",
        "\n",
        "The generator network also has 1 input layer, 1 hidden Dense layer of 32 neurons and 1 output layer.\n",
        "\n",
        "**Question** Can you find out how many trainable parameters the networks have?\n",
        "\n",
        "**Answer** Both the discriminator and the generataor network have 97 trainable parameters.\n",
        "\n",
        "**Question** What are the activation functions of both networks? Why are they like this?\n",
        "\n",
        "**Answer** Tha activation functions are both Leaky ReLU. A ReLU activation function is a very computationally simple and fast method that sets all negative values to zero, and returns all positive values unmodified. However, a big downside of a standard ReLU is that neurons get 'stuck' being set to zero, and are not further trained. To solve this, a leaky ReLU assigns very low negative values to the negative values, preventing vanishing gradients and allowing for stable convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACf058DjgZ9_"
      },
      "source": [
        "# For easier reading\n",
        "from keras.layers import Reshape, Dense, Dropout, Flatten\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "# The discriminator will directly classify the input value\n",
        "def get_discriminator_1D():\n",
        "  discriminator = keras.models.Sequential()\n",
        "  discriminator.add(Dense(32, input_dim=1))\n",
        "  discriminator.add(LeakyReLU())\n",
        "  discriminator.add(Dense(1, activation='sigmoid'))\n",
        "  return discriminator\n",
        "\n",
        "# The generator will transform a single input value\n",
        "def get_generator_1D():\n",
        "  generator = keras.models.Sequential()\n",
        "  generator.add(Dense(32, input_dim=1))\n",
        "  generator.add(LeakyReLU())\n",
        "  generator.add(Dense(1))\n",
        "  return generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QYrW32ehE3_"
      },
      "source": [
        "Now, we will define the training functions for both networks. Consider what is actually happening in a GAN and how the inputs and outputs are connected. There are three variables when training this GAN\n",
        "\n",
        "*   **z**: the noise that will be input to the generator\n",
        "*   **G_z**: the output of the generator, i.e. the samples that should approximate the real samples\n",
        "*   **D_G_z**: the discriminator's decision based on the fake sample\n",
        "\n",
        "The overall objective function of our system is as follows\n",
        "\n",
        "$V^{(D)}(D,G)=\\underset{x\\sim p_{data}}{\\mathbb{E}} [\\log{D(x)}]+\\underset{z\\sim p_z}{\\mathbb{E}} [\\log{(1-D(G(z)))}]$\n",
        "\n",
        "The generator $G$ is trying to minimize this loss, and the discriminator $D$ tries to maximize this. In other words, the discriminator wants to minimize the binary cross-entropy s.t. it predicts 1 for real samples and 0 for fake samples. At the same time, the generator tries to get the discriminator to predict 1 for fake samples.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H30WDcfXF7tY",
        "outputId": "7c6b94c5-f89c-4adf-8ce7-beff0b5da585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get networks\n",
        "discriminator = get_discriminator_1D()\n",
        "generator = get_generator_1D()\n",
        "\n",
        "# Configure both models for training\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "generator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "\n",
        "# To configure the full model, we will set the 'trainable' parameter of the discriminator to False, as we don't want to optimize the discriminator when optimizing the generator\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The input variable (noise) for the generator\n",
        "z = keras.layers.Input(shape=(latent_dim,))\n",
        "\n",
        "# What comes out of the generator\n",
        "G_z = generator(z)\n",
        "\n",
        "# What comes out of the discriminator when classifying the 'fake' samples\n",
        "D_G_z = discriminator(G_z)\n",
        "\n",
        "# The full GAN model\n",
        "gan = keras.models.Model(inputs=z, outputs=D_G_z)\n",
        "\n",
        "# The loss function for the GAN: this gets lower if the fake samples are classified as real\n",
        "gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator.summary()\n",
        "generator.summary()\n"
      ],
      "metadata": {
        "id": "thMVvOPCmrPU",
        "outputId": "b3f8d1fd-8268-4e63-9a66-409d25264f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │              \u001b[38;5;34m64\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m97\u001b[0m (388.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97</span> (388.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m97\u001b[0m (388.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97</span> (388.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │              \u001b[38;5;34m64\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m97\u001b[0m (388.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97</span> (388.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m97\u001b[0m (388.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97</span> (388.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MlpUJ_716Tk"
      },
      "source": [
        "The code below runs the training loop. This could take a while. The code will periodically show a plot of the current situation.\n",
        "\n",
        "**Question** Why do we set discriminator.trainable to either True or False?\n",
        "\n",
        "**Answer** When either the discriminator or generator is being trained, the other's weights are frozen. This prevents both from updating simultaneously, which would cause instability. When discriminator.trainable is set to True, only the discriminator's weights are updated, and the generator's weights are frozen. Conversely, when discriminator.trainable is set to False, the discriminator's weights are frozen, and the generator's weights are updated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynLndgse0r9V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "605ca692-404c-47b8-c36a-59b38f471cc8"
      },
      "source": [
        "# We will store the losses here\n",
        "g_losses = []\n",
        "d_losses = []\n",
        "\n",
        "# Training loop\n",
        "n_samples = 100\n",
        "epochs = 10000\n",
        "\n",
        "grid = widgets.Grid(1, 1)\n",
        "\n",
        "for e in range(epochs):\n",
        "  # Get a random set of input noise\n",
        "  noise = np.random.normal(0, 1, size=[n_samples, latent_dim])\n",
        "\n",
        "  # Also get a sample from the 'real' distribution\n",
        "  real = np.random.normal(real_mean, 1, size=[n_samples, latent_dim])\n",
        "\n",
        "  # Generate some fake samples using the generator\n",
        "  fake = generator.predict(noise)\n",
        "\n",
        "  # Concatenate the fake and real images\n",
        "  X = np.concatenate([real, fake])\n",
        "\n",
        "  # Labels for generated and real data\n",
        "  Y_dis = np.zeros(2*n_samples)\n",
        "\n",
        "  # Set labels for real samples to 1\n",
        "  Y_dis[:n_samples] = 1\n",
        "\n",
        "  # Train discriminator with this batch of samples\n",
        "  discriminator.trainable = True\n",
        "  d_loss = discriminator.train_on_batch(X, Y_dis)\n",
        "  d_losses.append(d_loss)\n",
        "\n",
        "  # Train generator with a new batch of generated samples\n",
        "  # Freeze the discriminator part\n",
        "  discriminator.trainable = False\n",
        "  noise = np.random.normal(0, 1, size=[n_samples, latent_dim])\n",
        "  # From the generator's perspective, the discriminator should predict\n",
        "  # ones for all samples\n",
        "  Y_gen = np.ones(n_samples)\n",
        "  g_loss = gan.train_on_batch(noise, Y_gen)\n",
        "  g_losses.append(g_loss)\n",
        "\n",
        "  if e % 100 == 0:\n",
        "    noise = np.random.normal(0, 1, size=[n_samples, latent_dim])\n",
        "    fake = generator.predict(noise)\n",
        "    real = np.random.normal(real_mean, 1, size=[n_samples, latent_dim])\n",
        "    pred = discriminator.predict(np.arange(-20, 20, 0.5).reshape((80, 1)))\n",
        "    with grid.output_to(0, 0):\n",
        "      grid.clear_cell()\n",
        "\n",
        "      # plt.clf()\n",
        "      pylab.hist((np.squeeze(fake), np.squeeze(real)), density=True, stacked=True)\n",
        "      pylab.scatter(np.arange(-20, 20, 0.5), pred, c='r')\n",
        "      pylab.xlim(-20, 20)\n",
        "      pylab.ylim(0, 1)\n",
        "      pylab.title('Iteration {}'.format(e))\n",
        "      pylab.legend(['Discriminator', 'Fake', 'Real'])\n",
        "    time.sleep(0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "       table#id2, #id2 > tbody > tr > th, #id2 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table id=id2><tr><td id=id2-0-0></td></tr></table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"88296f26-80c9-11ef-a0ae-0242ac1c000c\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_1ee340942e"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"882a2e84-80c9-11ef-a0ae-0242ac1c000c\"] = document.querySelector(\"#id2-0-0\");\n",
              "//# sourceURL=js_40b645a27c"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"882ae8c4-80c9-11ef-a0ae-0242ac1c000c\"] = google.colab.output.setActiveOutputArea(window[\"882a2e84-80c9-11ef-a0ae-0242ac1c000c\"]);\n",
              "//# sourceURL=js_ccfb2b5d49"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"883ac1e0-80c9-11ef-a0ae-0242ac1c000c\"] = google.colab.output.setActiveOutputArea(window[\"88296f26-80c9-11ef-a0ae-0242ac1c000c\"]);\n",
              "//# sourceURL=js_80204db45e"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7e4594140dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7e45880191b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbNFeFkSbXoN"
      },
      "source": [
        "If all is well, the fake and real distributions should nicely overlap after training. The discriminator has essentially pushed the fake samples towards the real distribution and the generator is now able to transform the noise distribution into a distribution of 'real' samples!\n",
        "\n",
        "**Question** Can you explain what happened to the red line during training? Why does it look like it does after training?\n",
        "\n",
        "**Question** Try training the GAN with different input noise distributions, e.g. uniform.\n",
        "\n",
        "**Question** See if you can find a distribution for the real samples for which the generator fails to generate samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuZBMvf4NWvD"
      },
      "source": [
        "During training, we have stored the loss values for the discriminator and the generator. We can now plot these. Remember that for the digit classification task, we were looking for a set of parameters leading to a low loss function.\n",
        "\n",
        "**Question** The loss curves that you get look different. Can you explain why they're not nicely dropping to zero? Can you explain the loss in the discriminator based on the objective function of the discriminator?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61bpDVDJNf12"
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(d_losses)\n",
        "plt.title('Discriminator loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(g_losses)\n",
        "plt.title('Generator loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dreog2u1b-iI"
      },
      "source": [
        "Although it is definitely nice that we can train two networks together to learn the distribution of a real data distribution, generating samples from a normal distribution is in itself not really interesting. Luckily, we can use the same principles to generate images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hq5i_sySZL1"
      },
      "source": [
        "# MNIST synthesis\n",
        "Like last week, we are again going to use MNIST data. Data preparation is the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYCMGnXnFnZc"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# load the MNIST the dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# scale the image intensities to the 0-1 range\n",
        "x_train = (x_train / 255.0).astype(np.float32)\n",
        "x_test = (x_test / 255.0).astype(np.float32)\n",
        "\n",
        "# convert the data to channel-last\n",
        "train_set_images = np.expand_dims(x_train, axis=-1)\n",
        "test_set_images = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "# convert the labels to one-hot encoded\n",
        "train_set_labels = to_categorical(y_train, num_classes=10)\n",
        "test_set_labels = to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-5ls6CemTIX"
      },
      "source": [
        "Plot some images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0M65_8qTmOI"
      },
      "source": [
        "def plotImages(images, dim=(10, 10), figsize=(10, 10), title=''):\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(images[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "plotImages(train_set_images[np.random.randint(0, train_set_images.shape[0], size=100)].reshape(100, 28, 28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJnlR4PJSeQB"
      },
      "source": [
        "In last week's exercise you have built a discriminative model that was able to classify an image into one of ten digit categories. In this exercise, we are going to do the inverse. Given a point in a latent space (which in our case will be a multi-dimensional Gaussian distribution), we are going to train the network to generate a realistic digit image for this point. The MNIST data set will be used as a set of real samples.\n",
        "\n",
        "<img src=\"https://skymind.ai/images/wiki/GANs.png\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blRfoIS4Uo8-"
      },
      "source": [
        "## The discriminator\n",
        "As you can see in the image above, we will need a generator and a discriminator network. Let's define these. Consider the network that you used\n",
        "for digit classification in the previous exercise and see if you can spot some differences between that network and the network below.\n",
        "\n",
        "**Question** Is this a convolutional neural network? Why (not)?\n",
        "\n",
        "**Answer** No the network below is not a convolutional neural network since it does not contain convolutional layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg36jdcyUvPF"
      },
      "source": [
        "def get_discriminator_MLP():\n",
        "  discriminator = keras.models.Sequential()\n",
        "  discriminator.add(Dense(1024, input_dim=784, kernel_initializer=keras.initializers.RandomNormal(stddev=0.02)))\n",
        "  discriminator.add(LeakyReLU(0.2))\n",
        "  discriminator.add(Dropout(0.3))\n",
        "  discriminator.add(Dense(512))\n",
        "  discriminator.add(LeakyReLU(0.2))\n",
        "  discriminator.add(Dropout(0.3))\n",
        "  discriminator.add(Dense(256))\n",
        "  discriminator.add(LeakyReLU(0.2))\n",
        "  discriminator.add(Dense(1, activation='sigmoid'))\n",
        "  return discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KYxSSbBQ28g"
      },
      "source": [
        "You could argue that digits are a bit more complex than samples from a Gaussian distribution, so let's set the latent space dimensionality for noise sampling a bit higher than 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmCVJxAmQ6nE"
      },
      "source": [
        "latent_dim = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BGSHlx-VmVY"
      },
      "source": [
        "\n",
        "## The generator\n",
        "\n",
        "The generator is different than the discriminator. It should go from a low-dimensional noise vector to an MNIST image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEjxnxAwVukq"
      },
      "source": [
        "def get_generator_MLP():\n",
        "  generator = keras.models.Sequential()\n",
        "  generator.add(Dense(256, input_dim=latent_dim, kernel_initializer=keras.initializers.RandomNormal(stddev=0.02)))\n",
        "  generator.add(LeakyReLU(0.2))\n",
        "  generator.add(Dense(512))\n",
        "  generator.add(LeakyReLU(0.2))\n",
        "  generator.add(Dense(1024))\n",
        "  generator.add(LeakyReLU(0.2))\n",
        "  generator.add(Dense(784, activation='tanh'))\n",
        "  return generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAUU4F7OQxO0"
      },
      "source": [
        "**Question** Consider the activation functions of the output layers of the generator and discriminator networks. How are they different?\n",
        "\n",
        "**Answer** The generator uses the 'tanh' activation function, while the discriminator employs 'sigmoid' at the output layer. The main difference is that 'tanh' outputs values between [−1,1], making it suitable for generating continuous data like images, whereas 'sigmoid' outputs values between [0,1], which is ideal for classification tasks and making a probability estimation.\n",
        "\n",
        "**Question** Also look at the activation functions of the other layers, can you find out what they do? Look at the Keras documentation.\n",
        "\n",
        "**Answer** The other layers use LeakyReLU as activation function. Unlike the standard ReLU function, which sets negative values to zero, LeakyReLU assigns a small value to these negative inputs. This prevents neurons from becoming inactive and no longer contributing to the learning process, as they would otherwise output zero if a normal ReLU function is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH_4zv4gnYhg"
      },
      "source": [
        "## The model\n",
        "Now  let's combine the generator and the discriminator. We train both using a binary crossentropy objective. This is very similar to what we did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dR_iHzXXHjb"
      },
      "source": [
        "discriminator = get_discriminator_MLP()\n",
        "generator = get_generator_MLP()\n",
        "\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "generator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "\n",
        "discriminator.trainable = False\n",
        "z = keras.layers.Input(shape=(latent_dim,))\n",
        "G_z = generator(z)\n",
        "D_G_z = discriminator(G_z)\n",
        "gan = keras.models.Model(inputs=z, outputs=D_G_z)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRMLacRF_RXo"
      },
      "source": [
        "Some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPXzOrqYZqmN"
      },
      "source": [
        "def saveModels(epoch):\n",
        "    generator.save('gan_generator_epoch_{}.h5'.format(epoch))\n",
        "    discriminator.save('gan_discriminator_epoch_{}.h5'.format(epoch))\n",
        "\n",
        "def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, latent_dim])\n",
        "    generatedImages = generator.predict(noise)\n",
        "    generatedImages = generatedImages.reshape(examples, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generatedImages.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Epoch {}'.format(epoch))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSJGSyUOmxCN"
      },
      "source": [
        "Run the code below to train the GAN model. Synthesized images should be shown periodically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYInR4WkXj_g"
      },
      "source": [
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "epochs = 200\n",
        "batch_size = 128\n",
        "\n",
        "X_train = (train_set_images.reshape(60000, 784).astype(np.float32) - 0.5)/0.5\n",
        "\n",
        "batch_count = int(X_train.shape[0] / batch_size)\n",
        "for e in range(epochs):\n",
        "  for _ in range(batch_count):\n",
        "\n",
        "    # Get a random set of input noise and images\n",
        "    noise = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
        "    image_batch = X_train[np.random.randint(0, X_train.shape[0], size=batch_size)]\n",
        "\n",
        "    # Generate some fake MNIST images using the generator\n",
        "    generated_images = generator.predict(noise)\n",
        "\n",
        "    # Concatenate the fake and real images\n",
        "    X = np.concatenate([image_batch, generated_images])\n",
        "\n",
        "    # Labels for generated and real data\n",
        "    y_dis = np.zeros(2*batch_size)\n",
        "    # One-sided label smoothing\n",
        "    y_dis[:batch_size] = 0.9\n",
        "\n",
        "    # Train discriminator with this batch of samples\n",
        "    discriminator.trainable = True\n",
        "    d_loss = discriminator.train_on_batch(X, y_dis)\n",
        "\n",
        "    # Train generator with a new batch of generated samples\n",
        "    noise = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
        "\n",
        "    # From the generator's perspective, the discriminator should predict\n",
        "    # ones for all samples\n",
        "    y_gen = np.ones(batch_size)\n",
        "\n",
        "    # Freeze the discriminator part\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # Train the GAN to predict ones\n",
        "    g_loss = gan.train_on_batch(noise, y_gen)\n",
        "\n",
        "  # Store loss of most recent batch from this epoch\n",
        "  d_losses.append(d_loss)\n",
        "  g_losses.append(g_loss)\n",
        "\n",
        "  if e % 5 == 0:\n",
        "    noise = np.random.normal(0, 1, size=[100, latent_dim])\n",
        "    plotGeneratedImages(e)\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    time.sleep(0.001)\n",
        "    saveModels(e)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkAQZ1-uA1SQ"
      },
      "source": [
        "**Question** Inspect the loss curves for this model and explain what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7MFDUeHSgQ-"
      },
      "source": [
        "## A convolutional model\n",
        "Thus far the discriminator and generator were both multilayer perceptrons. Now we're going to add in some convolutional layers to turn them into a deep convolutional GAN (<a href=\"http://arxiv.org/abs/1511.06434\">DCGAN</a>)-like architecture. This means that we have to redefine the generator network and a discriminator network.\n",
        "\n",
        "The discriminator network is (almost) the same network that we used in last week's exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bccrTGP0SiVJ"
      },
      "source": [
        "from keras.layers.convolutional import Conv2D, UpSampling2D\n",
        "\n",
        "def get_discriminator_CNN():\n",
        "  discriminator = keras.models.Sequential()\n",
        "  discriminator.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(1, 28, 28), kernel_initializer=keras.initializers.RandomNormal(stddev=0.02)))\n",
        "  discriminator.add(LeakyReLU(0.2))\n",
        "  discriminator.add(Dropout(0.3))\n",
        "  discriminator.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
        "  discriminator.add(LeakyReLU(0.2))\n",
        "  discriminator.add(Dropout(0.3))\n",
        "  discriminator.add(Flatten())\n",
        "  discriminator.add(Dense(1, activation='sigmoid'))\n",
        "  return discriminator\n",
        "\n",
        "def get_generator_CNN():\n",
        "  generator = keras.models.Sequential()\n",
        "  generator.add(Dense(128*7*7, input_dim=latent_dim, kernel_initializer=keras.initializers.RandomNormal(stddev=0.02)))\n",
        "  generator.add(LeakyReLU(0.2))\n",
        "  generator.add(Reshape((128, 7, 7)))\n",
        "  generator.add(UpSampling2D(size=(2, 2)))\n",
        "  generator.add(Conv2D(64, kernel_size=(5, 5), padding='same'))\n",
        "  generator.add(LeakyReLU(0.2))\n",
        "  generator.add(UpSampling2D(size=(2, 2)))\n",
        "  generator.add(Conv2D(1, kernel_size=(5, 5), padding='same', activation='tanh'))\n",
        "  return generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSKkoosKAwrh"
      },
      "source": [
        "Let's build our model like before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu8rmKcoGAPb"
      },
      "source": [
        "discriminator = get_discriminator_CNN()\n",
        "generator = get_generator_CNN()\n",
        "\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
        "generator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "discriminator.trainable = False\n",
        "z = keras.layers.Input(shape=(latent_dim,))\n",
        "x = generator(z)\n",
        "D_G_z = discriminator(x)\n",
        "gan = keras.models.Model(inputs=z, outputs=D_G_z)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojox3HxJnVDF"
      },
      "source": [
        "Train the model using the code below. Inspect the samples that come out.\n",
        "\n",
        "**Question** What are some differences between these samples and the ones generated by the multilayer perceptron GAN? Can you explain these differences?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k79EFqsgWz0l"
      },
      "source": [
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "epochs = 200\n",
        "batch_size = 128\n",
        "\n",
        "X_train = (train_set_images.reshape(60000, 1, 28, 28).astype(np.float32) - 0.5)/0.5\n",
        "\n",
        "batch_count = int(X_train.shape[0] / batch_size)\n",
        "for e in range(epochs):\n",
        "  for _ in range(batch_count):\n",
        "    # Get a random set of input noise and images\n",
        "    noise = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
        "    image_batch = X_train[np.random.randint(0, X_train.shape[0], size=batch_size)]\n",
        "\n",
        "    # Generate some fake MNIST images using the generator\n",
        "    generated_images = generator.predict(noise)\n",
        "\n",
        "    # Concatenate the fake and real images\n",
        "    X = np.concatenate([image_batch, generated_images])\n",
        "\n",
        "    # Labels for generated and real data\n",
        "    y_dis = np.zeros(2*batch_size)\n",
        "    # Set reference to 1 for real samples\n",
        "    y_dis[:batch_size] = 1\n",
        "\n",
        "    # Train discriminator with this batch of samples\n",
        "    discriminator.trainable = True\n",
        "    d_loss = discriminator.train_on_batch(X, y_dis)\n",
        "\n",
        "    # Train generator with a new batch of generated samples\n",
        "    noise = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
        "\n",
        "    # From the generator's perspective, the discriminator should predict\n",
        "    # ones for all samples\n",
        "    y_gen = np.ones(batch_size)\n",
        "\n",
        "    # Freeze the discriminator part\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # Train the GAN to predict ones\n",
        "    g_loss = gan.train_on_batch(noise, y_gen)\n",
        "\n",
        "    # Store loss of most recent batch from this epoch\n",
        "  d_losses.append(d_loss)\n",
        "  g_losses.append(g_loss)\n",
        "\n",
        "  if e % 5 == 0:\n",
        "    noise = np.random.normal(0, 1, size=[100, latent_dim])\n",
        "    generatedImages = generator.predict(noise)\n",
        "    generatedImages = generatedImages.reshape(100, 28, 28)\n",
        "    plotImages(generatedImages, title='Epoch {}'.format(e))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    time.sleep(0.001)\n",
        "    saveModels(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmtZkf5AnW6y"
      },
      "source": [
        "## Interpolation in the latent space\n",
        "We're going to explore the latent space a bit more. We pick two points in the latent space and make a linear interpolation between these two. Then we generate images from each of the interpolated latent points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NlTG4FBkNpC"
      },
      "source": [
        "noise_a = np.random.normal(0, 1, size=[1, latent_dim])\n",
        "noise_b = np.random.normal(0, 1, size=[1, latent_dim])\n",
        "\n",
        "noise = np.zeros((10, latent_dim), dtype='float32')\n",
        "for ni in range(10):\n",
        "  noise[ni, :] = float(ni)/10. * noise_a + (1 - float(ni)/10.) * noise_b\n",
        "generatedImages = generator.predict(noise)\n",
        "generatedImages = generatedImages.reshape(10, 28, 28)\n",
        "plotImages(generatedImages, dim=(1, 10), figsize=(10, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxkznblanlBO"
      },
      "source": [
        "**Question** Explain what you see in this plot.\n",
        "\n",
        "**Answer** The image on the far left resembles a \"7,\" while moving to the right showcases a gradual transformation. Through interpolation, the shape evolves, and a circular form starts to emerge at the top of the number. As a result, the image begins to look less like a \"7\" and more like a \"9\" or even a \"0.\"\n",
        "\n",
        "**Question** What happens when you extrapolate out of the latent space distribution? Consider how the noise vectors are drawn.\n",
        "\n",
        "**Answer** If you extrapolate outside of the trained latent space distribution, the images that will be generated will become unpredictable since it can result in images with features which are not in the trainingsset. This can lead to generated images which do not resemble any of the trainings data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEjdfhRZsUGR"
      },
      "source": [
        "# Histopathology image synthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhUnPg2sYitQ"
      },
      "source": [
        "Now we're going to synthesize some actual images. We're going to use the PatchCamelyon data set ([more info here](https://github.com/basveeling/pcam)) which consists of small images that can be used for classification benchmarks. Today, we're not going to classify images, but we're going to synthesize them. You can download a data set of smaller (28 x 28 pixels) versions of these images from\n",
        "[this link](https://drive.google.com/file/d/11tkBWleY6PqFAGQHW0zYOwhvuz0kq-ai/view?usp=sharing). Upload the data set in the dialog below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I9zf15C21kp",
        "outputId": "fcf63fb3-4397-457e-a1da-436239181313"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMGM8sLhsbWh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "6ce359cf-cec3-4dba-8a12-9294bedd209f"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-69c074e9-2e1d-4d9b-8da3-b1192eb47648\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-69c074e9-2e1d-4d9b-8da3-b1192eb47648\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving valid_28.pkl.gz to valid_28.pkl.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE8OlCi-ZKFf"
      },
      "source": [
        "Now load the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfJIE29rFYUA"
      },
      "source": [
        "def loadPatchCamelyon(path):\n",
        "    f = gzip.open(path, 'rb')\n",
        "    train_set = pickle.load(f, encoding='latin1')\n",
        "    f.close()\n",
        "    return train_set"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP1Eho1U1oMX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "dd082c1f-e1e5-4d14-a6aa-29712450a25a"
      },
      "source": [
        "train_set_images = loadPatchCamelyon(r'valid_28.pkl.gz')\n",
        "\n",
        "def plotImagesPatchCamelyon(images, dim=(10, 10), figsize=(10, 10), title=''):\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(images[i], interpolation='nearest')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plotImagesPatchCamelyon(train_set_images[np.random.randint(0, train_set_images.shape[0], size=100)].reshape(100, 28, 28, 3)/255.0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'loadPatchCamelyon' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f8a1be141510>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadPatchCamelyon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'valid_28.pkl.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplotImagesPatchCamelyon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loadPatchCamelyon' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQzcCZ_hZNbH"
      },
      "source": [
        "What do you immediately notice? Indeed, these images are in color! So the generator will have to generate three output channels instead of just one.\n",
        "\n",
        "In this second-to-last part of the practical you're going to repurpose the code that you have used so far to synthesize histopathology images like the ones above. You can play around a bit with this, see what happens when you interpolate between samples, etc. Remember that there is a final 'exercise' below.\n",
        "\n",
        "**Good luck!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCm1R35fSVlw"
      },
      "source": [
        "from keras.layers import Conv2D, UpSampling2D, LeakyReLU, Dropout, Flatten, Dense, Reshape\n",
        "\n",
        "def get_discriminator_CNN():\n",
        "  discriminator = keras.models.Sequential()\n",
        "  discriminator.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(3, 28, 28), kernel_initializer=keras.initializers.RandomNormal(stddev=0.02)))\n",
        "  discriminator.add(LeakyReLU(0.2))\n",
        "  discriminator.add(Dropout(0.3))\n",
        "  discriminator.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
        "  discriminator.add(LeakyReLU(0.2))\n",
        "  discriminator.add(Dropout(0.3))\n",
        "  discriminator.add(Flatten())\n",
        "  discriminator.add(Dense(1, activation='sigmoid'))\n",
        "  return discriminator\n",
        "\n",
        "def get_generator_CNN():\n",
        "  generator = keras.models.Sequential()\n",
        "  generator.add(Dense(128*7*7, input_dim=latent_dim, kernel_initializer=keras.initializers.RandomNormal(stddev=0.02)))\n",
        "  generator.add(LeakyReLU(0.2))\n",
        "  generator.add(Reshape((128, 7, 7)))\n",
        "  generator.add(UpSampling2D(size=(2, 2)))\n",
        "  generator.add(Conv2D(64, kernel_size=(5, 5), padding='same'))\n",
        "  generator.add(LeakyReLU(0.2))\n",
        "  generator.add(UpSampling2D(size=(2, 2)))\n",
        "  generator.add(Conv2D(3, kernel_size=(5, 5), padding='same', activation='tanh'))\n",
        "  return generator\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 50\n",
        "\n",
        "discriminator = get_discriminator_CNN()\n",
        "generator = get_generator_CNN()\n",
        "\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "generator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "\n",
        "discriminator.trainable = False\n",
        "z = keras.layers.Input(shape=(latent_dim,))\n",
        "x = generator(z)\n",
        "D_G_z = discriminator(x)\n",
        "gan = keras.models.Model(inputs=z, outputs=D_G_z)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))"
      ],
      "metadata": {
        "id": "O65kEGlR8bO0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "epochs = 200\n",
        "batch_size = 128\n",
        "\n",
        "X_train = (train_set_images.reshape(48000, 3, 28, 28).astype(np.float32) - 0.5)/0.5\n",
        "\n",
        "def plotImagesPatchCamelyon(images, dim=(10, 10), figsize=(10, 10), title=''):\n",
        "  plt.figure(figsize=figsize)\n",
        "  for i in range(images.shape[0]):\n",
        "      plt.subplot(dim[0], dim[1], i+1)\n",
        "      plt.imshow(images[i], interpolation='nearest')\n",
        "      plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.suptitle(title)\n",
        "  plt.show()\n",
        "\n",
        "batch_count = int(X_train.shape[0] / batch_size)\n",
        "for e in range(epochs):\n",
        "  for _ in range(batch_count):\n",
        "    # Get a random set of input noise and images\n",
        "    noise = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
        "    image_batch = X_train[np.random.randint(0, X_train.shape[0], size=batch_size)]\n",
        "\n",
        "    # Generate some fake MNIST images using the generator\n",
        "    generated_images = generator.predict(noise)\n",
        "\n",
        "    # Concatenate the fake and real images\n",
        "    X = np.concatenate([image_batch, generated_images])\n",
        "\n",
        "    # Labels for generated and real data\n",
        "    y_dis = np.zeros(2*batch_size)\n",
        "    # Set reference to 1 for real samples\n",
        "    y_dis[:batch_size] = 1\n",
        "\n",
        "    # Train discriminator with this batch of samples\n",
        "    discriminator.trainable = True\n",
        "    d_loss = discriminator.train_on_batch(X, y_dis)\n",
        "\n",
        "    # Train generator with a new batch of generated samples\n",
        "    noise = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
        "\n",
        "    # From the generator's perspective, the discriminator should predict\n",
        "    # ones for all samples\n",
        "    y_gen = np.ones(batch_size)\n",
        "\n",
        "    # Freeze the discriminator part\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # Train the GAN to predict ones\n",
        "    g_loss = gan.train_on_batch(noise, y_gen)\n",
        "\n",
        "    # Store loss of most recent batch from this epoch\n",
        "    d_losses.append(d_loss)\n",
        "    g_losses.append(g_loss)\n",
        "\n",
        "\n",
        "\n",
        "  if e % 5 == 0:\n",
        "    noise = np.random.normal(0, 1, size=[100, latent_dim])\n",
        "    generatedImages = generator.predict(noise)\n",
        "    generatedImages = generatedImages.reshape(100, 28, 28, 3)\n",
        "    plotImagesPatchCamelyon(generated_images[np.random.randint(0, generated_images.shape[0], size=100)].reshape(100, 28, 28, 3)/255.0)\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    time.sleep(0.001)\n",
        "    #saveModels(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Nn6oze9O8Y58",
        "outputId": "5dddd7b5-1efc-4a43-8cc9-8950560cb741"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_set_images' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9fbcf00ddcd6>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_set_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplotImagesPatchCamelyon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_set_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8CchPs2kIVT"
      },
      "source": [
        "# Conditional image synthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBALpqMLFPt2"
      },
      "source": [
        "## The BigGAN model\n",
        "The images we have synthesized so far are all quite small. Synthesis of larger images (e.g. 512 x 512 pixels) typically requires a lot of compute power and patience. Unfortunately, you don't have 100s of GPU at your disposal (I guess) to train a model like BigGAN, the state of the art in conditional image synthesis. A wild guess is that it would cost you around [USD60000](https://twitter.com/quasimondo/status/1065610256917692416) to train this model.\n",
        "\n",
        "Luckily, the authors of BigGAN have put a version of their pretrained model online and you can use it to synthesize images. Go to [this Colaboratory file](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb) and answer the following questions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question** What happens when you change the noise seed? Also try out different categories."
      ],
      "metadata": {
        "id": "BqDhv_3YIWMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer** A BigGAN is conditional, it takes a latent noise vector and a category label as input. In GANs, the noise seed is used to generate different random latent vectors. These latent vectors are inputs for the generator. If the noise seed is changed, the image that the generator produces will be different. Additionally, the category label allows you to choose within which category the generator makes images, for example, dog or cat.\n",
        "\n",
        "So: changing noise seed within the same category will give different images of the same category, e.g. different images of dogs. Changing the category will switch to different objects, like cats."
      ],
      "metadata": {
        "id": "WjUhwswGIxCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question** What happens to your samples when you change the truncation value? More precisely, what happens to the diversity and the quality of your samples? Take a look at the [paper on arXiv](https://arxiv.org/abs/1809.11096) Sec. 3.1 to get an idea what this value does."
      ],
      "metadata": {
        "id": "HBRCYft0IWoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer** The truncation in BigGAN controls how much the latent space is constrained or truncated, which gives the trade-off between image diversity and quality. Low truncation values (nearing zero) give a constrained noise vector, which gives a high-quality, realistic image. The downside is that these images lack diversity. On the other hand, a high truncation value (1 or higher) increases diversity but often gives low quality and unrealistic images."
      ],
      "metadata": {
        "id": "OtfmxUqdI0IV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question** Interpolate between image categories, inspect what these look like."
      ],
      "metadata": {
        "id": "lD_zQ2rSIjaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer** Interpolating between image categories means generating images that are a mix of two categories, like dogs, cats or cars. These interpolations will result in a blend of the two categories, with characteristics of both objects, or a completely abstract image.\n"
      ],
      "metadata": {
        "id": "6PO9lPmQI2Po"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqKCNciykQ-2"
      },
      "source": [
        "## Conditional MNIST synthesis\n",
        "For all MNIST samples we already have labels (0, 1, ..., n). Try to change the MNIST synthesis code such that you can ask the generator to generate specific labels. I.e., try to train a conditional GAN. You can look for some inspiration in [this paper](https://arxiv.org/pdf/1411.1784.pdf), in particular Sec. 4.1. Remember that you already got the MNIST labels when loading the data set."
      ]
    }
  ]
}