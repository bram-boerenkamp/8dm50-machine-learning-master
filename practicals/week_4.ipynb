{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "In this practical session you will implement and train several Convolutional Neural Networks (CNNs) using the Keras framework with a Tensorflow backend. If you are not already familiar with Keras, you can go over the [following tutorial](https://github.com/tueimage/essential-skills/blob/master/keras.md). More detailed information on the different functionalities can be found in the [Keras library documentation](https://keras.io/). \n",
    "\n",
    "Note that for this set of exercise CPU-only Tensorflow, which you should already have installed, is sufficient (i.e. GPU-support is not required but it will make your experiments run faster). \n",
    "\n",
    "You are also required to use the `gryds` package for data augmentation that you can install directly from git: `pip install git+https://github.com/tueimage/gryds/`.\n",
    "\n",
    "You also have to install the Keras deep learning framework (if you have not done so already) by running `conda install keras`. Note that there are two implementations of Keras, one from https://keras.io/ and another one that ships with Tensorflow. Here we use the former. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "We will first train a simple CNN to classify handwritten digits using the MNIST dataset. This dataset is often referred to as the \"Hello world!\" example of deep learning because it can be used to quickly illustrate a small neural network in action (and obtain a decent classification accuracy in the process). More information on it can be found [here](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "First, let's load the dataset and visualize some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# load the MNIST the dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# scale the image intensities to the 0-1 range\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "# convert the data to channel-last\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# convert the labels to one-hot encoded\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "def plot_images(images, dim=(10, 10), figsize=(10, 10), title=''):\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_images(x_train[np.random.randint(0, x_train.shape[0], size=100)].reshape(100, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST classification task is quite simple: given an image, predict the digit that it contains. Thus, this is a 10-class classification problem.\n",
    "\n",
    "Let's define a simple network for the handwritten digit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and train the network (note that this could take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate its performance on the independent test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net\n",
    "The U-Net convolutional neural network architecture was first developed for biomedical image segmentation and is to this day one of the most widely used methods for image segmentation. The details of the architecture can be found in the [original paper](https://arxiv.org/abs/1505.04597). In this practical we will build and train a U-Net network that is able to segment blood vessels in retinal images. \n",
    "\n",
    "### Loading and visualizing the data\n",
    "The data for this task is taken from the [DRIVE](https://www.isi.uu.nl/Research/Databases/DRIVE/index.php) database. It consists of photographs of the retina, where the goal is to segment the blood vessels within. The dataset has a total of 40 photographs, divided in 20 images for training and 20 for testing. The images corresponding to the DRIVE test set can be found [here](https://www.dropbox.com/s/zk51wgupimw7jd9/DRIVE.zip?dl=0).\n",
    "\n",
    "Let's load the training set and visualize an image with the corresponding blood vessel segmentation. For training we will divide the data in a training and a validation set to monitor the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "\n",
    "sys.path.append('code/')\n",
    "from unet_utils import load_data\n",
    "\n",
    "# location of the DRIVE dataset\n",
    "data_folder = 'data/DRIVE/'\n",
    "train_paths = glob(data_folder + 'training/images/*.tif')\n",
    "images, masks, segmentations = load_data(train_paths)\n",
    "\n",
    "# print the shape of image dataset\n",
    "print(images.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Image of the retina\")\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Ground truth vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(segmentations[0][:, :, 0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# divide in training and validation\n",
    "train_images, val_images, train_masks, val_masks, train_segmentations, val_segmentations = train_test_split(\n",
    "    images, masks, segmentations, test_size=0.2, random_state=7)\n",
    "\n",
    "# print the shape of the training and valudation datasets\n",
    "print(train_images.shape)\n",
    "print(train_masks.shape)\n",
    "print(train_segmentations.shape)\n",
    "print(val_images.shape)\n",
    "print(val_masks.shape)\n",
    "print(val_segmentations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a U-Net model\n",
    "\n",
    "You are already provided with implementation of the U-Net architecture in `unet_model.py`. This is a modular implementation and can be used to generate U-Net architectures with a variety of hyperparameters such as depth and number of feature maps. Before using the model, examine the code and documentation and make sure that you understand all the details.\n",
    "\n",
    "We will train a U-Net model using smaller patches extracted from the training images. Training the images on smaller patches requires less computation power and results in a more varied training dataset (it has the effect of data augmentation by image translation). Because a U-Net is a fully convolutional network it can be evaluated on inputs of different size (the output size will change according to the input size). Thus, although the model will be trained on smaller patches it can still be used to segment larger images with one pass through the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_utils import extract_patches, preprocessing\n",
    "\n",
    "# work with 32x32 patches\n",
    "patch_size = (32, 32)\n",
    "\n",
    "# 200 patches per image\n",
    "patches_per_im = 200\n",
    "\n",
    "# visualize a couple of patches as a visual check\n",
    "patches, patches_segmentations = extract_patches(train_images, train_segmentations, patch_size, patches_per_im=1, seed=7)\n",
    "\n",
    "print(patches.shape)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 12))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 0].imshow(patches[i])\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 1].imshow(patches_segmentations[i][:, :, 0])\n",
    "    axes[i, 2].axis('off')\n",
    "    axes[i, 2].imshow(patches[i+5])\n",
    "    axes[i, 3].axis('off')\n",
    "    axes[i, 3].imshow(patches_segmentations[i+5][:, :, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the validation data to fit the U-Net model\n",
    "# images of shape (584, 565) shape result in concatenation error due to the odd number of columns\n",
    "\n",
    "print(\"Old shape:\", val_images.shape)\n",
    "\n",
    "val_images, val_masks, val_segmentations = preprocessing(\n",
    "    val_images, \n",
    "    val_masks, \n",
    "    val_segmentations, \n",
    "    desired_shape=(584, 584))\n",
    "    \n",
    "print(\"New shape:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle  \n",
    "from unet_model import unet\n",
    "from unet_utils import datagenerator\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# use a single training image, to better demonstrate the effects of data augmentation\n",
    "X_train, y_train = np.expand_dims(train_images[0], axis=0), np.expand_dims(train_segmentations[0], axis=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# hyperparameters\n",
    "depth = 3\n",
    "channels = 32\n",
    "use_batchnorm = True\n",
    "batch_size = 64\n",
    "epochs = 250\n",
    "steps_per_epoch = int(np.ceil((patches_per_im * len(train_images)) / batch_size))\n",
    "\n",
    "# work with 32x32 patches\n",
    "patch_size = (32, 32)\n",
    "# 200 patches per image\n",
    "patches_per_im = 200\n",
    "\n",
    "# initialize model\n",
    "model = unet(input_shape=(None, None, 3), depth=depth, channels=channels, batchnorm=use_batchnorm)\n",
    "\n",
    "# print a summary of the model\n",
    "# model.summary(line_length=120)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# stop the training if the validation loss does not increase for 15 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# train the model with the data generator, and save the training history\n",
    "history = model.fit(datagenerator(X_train, y_train, patch_size, patches_per_im, batch_size),\n",
    "                              validation_data=(val_images, val_segmentations),\n",
    "                              steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=2,\n",
    "                              callbacks=[early_stopping])\n",
    "\n",
    "# Save training history to a file using pickle\n",
    "history_file_path = 'training_history.pkl'  # Change path as necessary\n",
    "with open(history_file_path, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "print(f\"Training history saved to {history_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model on one test image and show the results\n",
    "from unet_utils import preprocessing\n",
    "\n",
    "# test data paths\n",
    "impaths_test = glob(data_folder + 'test/images/*.tif')\n",
    "\n",
    "# load data\n",
    "test_images, test_masks, test_segmentations = load_data(impaths_test, test=True)\n",
    "\n",
    "# pad the data to fit the U-Net model\n",
    "test_images, test_masks, test_segmentations = preprocessing(test_images, test_masks, test_segmentations, \n",
    "                                                            desired_shape=(584, 584))\n",
    "\n",
    "# use a single image to evaluate\n",
    "X_test, y_test = np.expand_dims(test_images[0], axis=0), np.expand_dims(test_masks[0], axis=0)\n",
    "\n",
    "# predict test samples\n",
    "test_prediction = model.predict(X_test, batch_size=4)\n",
    "\n",
    "# visualize the test result\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Image of the retina\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_images[0])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Ground truth vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_segmentations[0][:, :, 0])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_prediction[0, :, :, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Number of parameters\n",
    "\n",
    "The first convolutional layer in the MNIST example has 320 parameters. The first fully connected layer has 1179,776 parameters. What do these parameters correspond to? \n",
    "\n",
    "<font color='#770a0a'>What is the general expression for the number of parameters of 1) a convolutional layer and 2) a fully-connected layer?</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwser\n",
    "\n",
    "##### The first convolutional layer in the MNIST example has 320 parameters. The first fully connected layer has 1,179,776 parameters. What do these parameters correspond to?\n",
    "\n",
    "The parameters given by `model.summary()` correspond to weights and biases that the model learns during the training process. The number of the parameters depends on the size and amount of filters used in a specific layer. For the first convolutional layer we have 320 parameters because:\n",
    "\n",
    "* There are 32 filters (last value of the layer shape)\n",
    "* the filters are 3x3x1 in dimension so there are 9 weights per filter\n",
    "* There are 32 biases, one for each filter\n",
    "\n",
    "This leads to 32 x 9 + 32 = 320 parameters.\n",
    "\n",
    "for the first fully connected layer we do the same type of calculation:\n",
    "\n",
    "* The layer before the first fully connected layer has 9,216 inputs.\n",
    "* The first fully connected layer has 128 neurons.\n",
    "* Each neuron has a bias so there are 128 biases.\n",
    "\n",
    "since each input is connected to a neuron (hence fully connected layer) the total number of parameters is:\n",
    "\n",
    "9,216 x 128 + 128 = 1,179,776 parameters\n",
    "\n",
    "##### What is the general expression for the number of parameters of 1) a convolutional layer and 2) a fully-connected layer?\n",
    "\n",
    "1) For a convolution layer the general expression for the number of parameters is dependent on several factors namely, filter size (FxF), number of input channels (RGB or greyscale, C), number of filters (N) and the biases of the filters (1 per filter). This leads to the following expression:\n",
    "\n",
    "$N_{parameters} = (F*F*C+1)*N_{filters}$\n",
    "\n",
    "2) For the fully connected layer this expression is slightly different here the number of parameters is dependent on: the number of inputs (I), the number of neurons or outputs (O) and the biases of these outputs (also O).\n",
    "This leads to:\n",
    "\n",
    "$N_{parameters} = (I*O)+O$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-convolutional MNIST model\n",
    "\n",
    "Modify the model in the MNIST example in such a way that it only contains convolutional layers while keeping the same number of parameters. If you do the modification correctly, the two models will have the same behaviour (i.e. they will represent the same model, only with different implementation). Show this experimentally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE NETWORK\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "model_c = Sequential()\n",
    "model_c.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model_c.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_c.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_c.add(Dropout(0.25))\n",
    "model_c.add(Conv2D(128, (12, 12), activation='relu'))\n",
    "model_c.add(Dropout(0.5))\n",
    "model_c.add(Conv2D(10, (1, 1), activation='softmax'))\n",
    "model_c.add(GlobalAveragePooling2D())\n",
    "\n",
    "model_c.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE AND TRAIN NETWORK\n",
    "from keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "model_c.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "model_c.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score_c = model_c.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score_c[0])\n",
    "print('Test accuracy:', score_c[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fully convolutional network gives a test loss of 0,03 and a test accuracy of 0.99, similar to the model with a dense layer which gives a test loss of 0.029 and a test accuracy of 0.99. This shows that the 2 models perform similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net architecture\n",
    "\n",
    "<font color='#770a0a'> What is the role of the skip connections in the U-Net neural network architecture? Will it be possible to train the exact same architecture with the skip connections omitted? If yes, what would be the expected result? If no, what would be the cause of the error?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "##### What is the role of the skip connections in the U-Net neural network architecture?\n",
    "\n",
    "The skip connections have two roles in the U-Net:\n",
    "\n",
    "* Fine-grained detail recovery\n",
    "The role of skip connections in the U-Net architecture is to recover fine-grained details in the prediction. During the encoding phase, the spatial resolution is reduced, losing detailed information about the original input. Skip connections link the corresponding layers in the encoder and decoder, allowing the decoder to access high-resolution features from the encoder. This enables the network to keep fine details and make more accurate, pixel-wise predictions in the final output.\n",
    "\n",
    "* Vanishing gradient prevention\n",
    "Skip connections help prevent the vanishing gradient problem, which can occur in deeper networks. By allowing the gradient to flow directly from deeper layers to shallower layers during backpropagation, skip connections maintain stronger gradients. This helps the network to train more effectively. Without them, the network might struggle to update its earlier layers, leading to poor performance.\n",
    "\n",
    "##### Will it be possible to train the exact same architecture with the skip connections omitted? If yes, what would be the expected result? If no, what would be the cause of the error?\n",
    "\n",
    "Yes, it is possible to train the same architecture without skip connections. However, the expected result would be less performance.\n",
    "\n",
    "Without skip connections, the model would lose the ability to pass the fine-grained details from the encoder to the decoder. As a result, the model may generate predictions that are blurred or less accurate. This is especially a problem in tasks that require pixel-level accuracy, like segmentation.\n",
    "Additionally, the absence of skip connections may result in the vanishing gradient problem, making it harder for the network to train effectively. This can lead to slower convergence or even not training at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "<font color='#770a0a'>Why does data augmentation result in less overfitting? Can data augmentation be applied to the test samples? If yes, towards what goal? If no, what is preventing that?</font>\n",
    "\n",
    "\n",
    "Implement random brightness augmentation of the image data by adding a random offset to the image intensity before passing them trough the network at training time. Train a model with random brightness augmentation and compare it to the baseline above. \n",
    "\n",
    "Implement data augmentation procedure that in addition to brightness augmentation also performs b-spline geometric augmentation using the [`gryds`](https://github.com/tueimage/gryds) package (you can look at the documentation of the package for an example on how to do that). Compare the new model with the baseline and the model that only performs brightness augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Data augmentation increases the diversity of the trainingsdata and can also expand the size of the trainingsdataset. Because of that, it helps the model to learn more generalized features, which in turn reduces the risk of overfitting.\n",
    "\n",
    "However, it is not suitable for the test dataset since the goal of the test dataset is to represent real-world data and evaluate how well the model performs on new, unseen data. Augmented data is artificially genereated and is therefore not in its original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "sys.path.append('code/')\n",
    "from unet_utils import load_data, extract_patches\n",
    "\n",
    "# Load data from the DRIVE dataset\n",
    "data_folder = 'data/DRIVE/'\n",
    "train_paths = glob(data_folder + 'training/images/*.tif')\n",
    "images, masks, segmentations = load_data(train_paths)\n",
    "\n",
    "# Print the shape of the image dataset\n",
    "print(\"Images shape:\", images.shape)\n",
    "\n",
    "# Visualize a sample image and its corresponding segmentation\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Image of the retina\")\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Ground truth vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(segmentations[0][:, :, 0])\n",
    "plt.show()\n",
    "\n",
    "train_images, val_images, train_masks, val_masks, train_segmentations, val_segmentations = train_test_split(\n",
    "    images, masks, segmentations, test_size=0.2, random_state=7)\n",
    "\n",
    "def random_brightness(image, max_delta=0.4):\n",
    "    delta = random.uniform(-max_delta, max_delta)\n",
    "    augmented_image = np.clip(image + delta, 0, 1)  \n",
    "    return augmented_image\n",
    "\n",
    "augmented_images = np.array([random_brightness(img) for img in train_images])\n",
    "train_images_augmented = np.concatenate([train_images, augmented_images], axis=0)\n",
    "train_segmentations_augmented = np.concatenate([train_segmentations, train_segmentations], axis=0)\n",
    "\n",
    "# Visualize original and augmented images\n",
    "num_samples = 8  # Number of samples to visualize\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Original images\n",
    "    plt.subplot(num_samples, 2, i * 2 + 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(train_images[i])\n",
    "    \n",
    "    # Augmented images\n",
    "    plt.subplot(num_samples, 2, i * 2 + 2)\n",
    "    plt.title(\"Augmented Image\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(augmented_images[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "patch_size = (32, 32)\n",
    "patches_per_im = 200\n",
    "\n",
    "patches, patches_segmentations = extract_patches(augmented_images, train_segmentations, patch_size, patches_per_im=1, seed=7)\n",
    "\n",
    "print(\"Patches shape:\", patches.shape)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 12))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 0].imshow(patches[i])\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 1].imshow(patches_segmentations[i][:, :, 0])\n",
    "    axes[i, 2].axis('off')\n",
    "    axes[i, 2].imshow(patches[i + 5])  \n",
    "    axes[i, 3].axis('off')\n",
    "    axes[i, 3].imshow(patches_segmentations[i + 5][:, :, 0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Old shape:\", val_images.shape)\n",
    "\n",
    "val_images, val_masks, val_segmentations = preprocessing(\n",
    "    val_images, \n",
    "    val_masks, \n",
    "    val_segmentations, \n",
    "    desired_shape=(584, 584))\n",
    "    \n",
    "print(\"New shape:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from unet_model import unet\n",
    "from unet_utils import datagenerator\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Hyperparameters\n",
    "depth = 3\n",
    "channels = 32\n",
    "use_batchnorm = True\n",
    "batch_size = 64\n",
    "epochs = 250\n",
    "patches_per_im = 200\n",
    "steps_per_epoch = int(np.ceil((patches_per_im * len(train_images)) / batch_size))\n",
    "\n",
    "model = unet(input_shape=(None, None, 3), depth=depth, channels=channels, batchnorm=use_batchnorm)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Stop the training if the validation loss does not increase for 15 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Train the model with the data generator, and save the training history\n",
    "history = model.fit(datagenerator(train_images_augmented, train_segmentations_augmented, patch_size, patches_per_im, batch_size),\n",
    "                    validation_data=(val_images, val_segmentations),\n",
    "                    steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=2,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "history_file_path = 'training_history1.pkl' \n",
    "with open(history_file_path, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "print(f\"Training history saved to {history_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With B-spline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "import random\n",
    "import gryds  \n",
    "from unet_utils import load_data, extract_patches\n",
    "\n",
    "data_folder = 'data/DRIVE/'\n",
    "train_paths = glob(data_folder + 'training/images/*.tif')\n",
    "images, masks, segmentations = load_data(train_paths)\n",
    "\n",
    "print(\"Images shape:\", images.shape)\n",
    "\n",
    "# Split into training and validation datasets\n",
    "train_images, val_images, train_masks, val_masks, train_segmentations, val_segmentations = train_test_split(\n",
    "    images, masks, segmentations, test_size=0.2, random_state=7)\n",
    "\n",
    "# Function for random brightness augmentation\n",
    "def random_brightness(image, max_delta=0.4):\n",
    "    delta = random.uniform(-max_delta, max_delta)\n",
    "    augmented_image = np.clip(image + delta, 0, 1)  \n",
    "    return augmented_image\n",
    "\n",
    "# Function for B-spline geometric augmentation using gryds\n",
    "def b_spline_augmentation(image, grid_size=(3, 3), max_deformation=0.1):\n",
    "    random_grid = np.random.rand(2, grid_size[0], grid_size[1])\n",
    "    random_grid -= 0.5  \n",
    "    random_grid /= 5    \n",
    "    transformed_channels = []\n",
    "\n",
    "    for channel in range(image.shape[2]):\n",
    "        bspline = gryds.BSplineTransformation(random_grid)\n",
    "        interpolator = gryds.Interpolator(image[:, :, channel])\n",
    "        transformed_channel = interpolator.transform(bspline)\n",
    "        transformed_channels.append(transformed_channel)\n",
    "    transformed_image = np.stack(transformed_channels, axis=-1)\n",
    "\n",
    "    return np.clip(transformed_image, 0, 1)  # Ensure values are in [0, 1]\n",
    "\n",
    "# Combined augmentation function\n",
    "def augment_image(image):\n",
    "    bright_img = random_brightness(image)\n",
    "    return b_spline_augmentation(bright_img)\n",
    "\n",
    "augmented_train_images = np.array([augment_image(img) for img in train_images])\n",
    "data_2 = augmented_train_images\n",
    "\n",
    "# Visualize original and augmented images\n",
    "num_samples = 4  # Number of samples to visualize\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(num_samples, 2, i * 2 + 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.subplot(num_samples, 2, i * 2 + 2)\n",
    "    plt.title(\"Augmented Image\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(data_2[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "patch_size = (32, 32)\n",
    "patches_per_im = 200\n",
    "\n",
    "patches, patches_segmentations = extract_patches(data_2, train_segmentations, patch_size, patches_per_im=1, seed=7)\n",
    "\n",
    "print(\"Patches shape:\", patches.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Old shape:\", val_images.shape)\n",
    "\n",
    "val_images, val_masks, val_segmentations = preprocessing(\n",
    "    val_images, \n",
    "    val_masks, \n",
    "    val_segmentations, \n",
    "    desired_shape=(584, 584))\n",
    "    \n",
    "print(\"New shape:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from unet_model import unet\n",
    "from unet_utils import datagenerator\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Prepare augmented data\n",
    "X_train_aug = np.expand_dims(data_2, axis=0)  # Use data_2 directly\n",
    "y_train_aug = np.expand_dims(train_segmentations, axis=0)  \n",
    "\n",
    "print(\"Training image shape:\", X_train_aug.shape)\n",
    "print(\"Training segmentation shape:\", y_train_aug.shape)\n",
    "\n",
    "depth = 3\n",
    "channels = 32\n",
    "use_batchnorm = True\n",
    "batch_size = 64\n",
    "epochs = 250\n",
    "patches_per_im = 200  \n",
    "steps_per_epoch = int(np.ceil((patches_per_im * len(data_2)) / batch_size)) \n",
    "\n",
    "model = unet(input_shape=(None, None, 3), depth=depth, channels=channels, batchnorm=use_batchnorm)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Stop the training if the validation loss does not increase for 15 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Train the model with the data generator using augmented data\n",
    "history = model.fit(datagenerator(data_2, train_segmentations, patch_size, patches_per_im, batch_size),\n",
    "                    validation_data=(val_images, val_segmentations),\n",
    "                    steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=2,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "history_file_path = 'training_history2.pkl'  \n",
    "with open(history_file_path, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "print(f\"Training history saved to {history_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with open('training_history.pkl', 'rb') as file:\n",
    "    training_history = pickle.load(file)\n",
    "\n",
    "with open('training_history1.pkl', 'rb') as file:\n",
    "    training_history1 = pickle.load(file)\n",
    "\n",
    "with open('training_history2.pkl', 'rb') as file:\n",
    "    training_history2 = pickle.load(file)\n",
    "\n",
    "\n",
    "def plot_multiple_histories_with_min_max(histories, labels, save_path='combined_training_history_plot.png'):\n",
    "    \"\"\"Plot training and validation loss/accuracy separately and show min/max for losses and accuracies.\"\"\"\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    plt.subplot(3, 2, 1)\n",
    "    for history, label in zip(histories, labels):\n",
    "        epochs = range(len(history['loss'])) \n",
    "        plt.plot(epochs, history['loss'], label=f'{label} - Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 2, 2)\n",
    "    for history, label in zip(histories, labels):\n",
    "        epochs = range(len(history['val_loss']))  \n",
    "        plt.plot(epochs, history['val_loss'], label=f'{label} - Validation Loss')\n",
    "    plt.title('Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training accuracy for all histories\n",
    "    plt.subplot(3, 2, 3)\n",
    "    for history, label in zip(histories, labels):\n",
    "        epochs = range(len(history['accuracy']))  # Adjust epochs length for each history\n",
    "        plt.plot(epochs, history['accuracy'], label=f'{label} - Training Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 2, 4)\n",
    "    for history, label in zip(histories, labels):\n",
    "        epochs = range(len(history['val_accuracy']))  \n",
    "        plt.plot(epochs, history['val_accuracy'], label=f'{label} - Validation Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    min_training_losses = [np.min(history['loss']) for history in histories]\n",
    "    min_val_losses = [np.min(history['val_loss']) for history in histories]\n",
    "    max_training_accuracies = [np.max(history['accuracy']) for history in histories]\n",
    "    max_val_accuracies = [np.max(history['val_accuracy']) for history in histories]\n",
    "\n",
    "    plt.subplot(3, 2, 5)\n",
    "    bar_width = 0.2\n",
    "    labels_pos = np.arange(len(labels))\n",
    "\n",
    "    plt.bar(labels_pos, min_training_losses, bar_width, label='Min Training Loss', color='lightblue')\n",
    "    plt.bar(labels_pos + bar_width, min_val_losses, bar_width, label='Min Validation Loss', color='lightgreen')\n",
    "    plt.xticks(labels_pos + bar_width / 2, labels)\n",
    "    plt.title('Lowest Loss Comparison')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.bar(labels_pos, max_training_accuracies, bar_width, label='Max Training Accuracy', color='lightcoral')\n",
    "    plt.bar(labels_pos + bar_width, max_val_accuracies, bar_width, label='Max Validation Accuracy', color='lightseagreen')\n",
    "    plt.xticks(labels_pos + bar_width / 2, labels)\n",
    "    plt.title('Highest Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    print(f\"Combined training history plot saved to {save_path}\")\n",
    "\n",
    "histories = [training_history, training_history1, training_history2]\n",
    "labels = ['No data augmentation', 'Brightness augmentation', 'Brightness and B-spline augmentation']\n",
    "\n",
    "# Plot the histories\n",
    "plot_multiple_histories_with_min_max(histories, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
